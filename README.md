# âš™ï¸ **Data Processing â€” MLOps Machine Maintenance**

This branch builds upon the **initial setup** by introducing the **`data_processing.py`** module inside `src/`.
It marks the **first executable workflow stage** of the **MLOps Machine Maintenance** pipeline â€” responsible for loading raw machine sensor data, cleaning and transforming it, encoding categorical features, scaling numerical inputs, and saving train/test splits for model training and predictive maintenance analysis.

## ğŸ§© **Overview**

The `DataProcessing` class implements a **deterministic preprocessing workflow** with integrated logging and unified exception handling.
It produces reproducible, well-structured datasets ready for downstream models that predict machine efficiency or failure risk.

### ğŸ” Core Responsibilities

| Stage | Operation          | Description                                                                                           |
| ----: | ------------------ | ----------------------------------------------------------------------------------------------------- |
|   1ï¸âƒ£ | **Load Data**      | Reads input CSV from `artifacts/raw/data.csv`.                                                        |
|   2ï¸âƒ£ | **Preprocess**     | Parses `Timestamp`, derives `Year`, `Month`, `Day`, and `Hour`, and label-encodes categorical fields. |
|   3ï¸âƒ£ | **Scale Features** | Standardises numeric features with `StandardScaler` to normalise input magnitudes.                    |
|   4ï¸âƒ£ | **Split Data**     | Performs an 80/20 **stratified** train/test split on `Efficiency_Status`.                             |
|   5ï¸âƒ£ | **Save Artefacts** | Persists scaled splits and the fitted scaler into `artifacts/processed/`.                             |

## ğŸ—‚ï¸ **Updated Project Structure**

```text
mlops_machine_maintenance/
â”œâ”€â”€ .venv/                          # ğŸ§© Local virtual environment (created by uv)
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ data.csv                # âš™ï¸ Input sensor dataset
â”‚   â””â”€â”€ processed/                  # ğŸ’¾ Processed output artefacts
â”‚       â”œâ”€â”€ X_train.pkl
â”‚       â”œâ”€â”€ X_test.pkl
â”‚       â”œâ”€â”€ y_train.pkl
â”‚       â”œâ”€â”€ y_test.pkl
â”‚       â””â”€â”€ scaler.pkl
â”œâ”€â”€ mlops_machine_maintenance.egg-info/ # ğŸ“¦ Package metadata (auto-generated)
â”œâ”€â”€ pipeline/                       # âš™ï¸ Pipeline orchestration (future stage)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ custom_exception.py         # Unified and detailed exception handling
â”‚   â”œâ”€â”€ logger.py                   # Centralised logging configuration
â”‚   â””â”€â”€ data_processing.py          # ğŸ§  End-to-end sensor data preparation
â”œâ”€â”€ static/                         # ğŸ“Š Visual or diagnostic assets
â”œâ”€â”€ templates/                      # ğŸ§© Placeholder for web/API templates
â”œâ”€â”€ .gitignore                      # ğŸš« Git ignore rules
â”œâ”€â”€ .python-version                 # ğŸ Python version pin
â”œâ”€â”€ pyproject.toml                  # âš™ï¸ Project metadata and uv configuration
â”œâ”€â”€ requirements.txt                # ğŸ“¦ Python dependencies
â”œâ”€â”€ setup.py                        # ğŸ”§ Editable install support
â””â”€â”€ uv.lock                         # ğŸ”’ Locked dependency versions
```

## âš™ï¸ **How to Run the Data Processing Module**

After activating the virtual environment and ensuring your dataset is located at `artifacts/raw/data.csv`, run:

```bash
python src/data_processing.py
```

### âœ… **Expected Successful Output**

```console
2025-11-08 14:02:11,213 - INFO - Data loaded successfully. Shape: (5000, 15)
2025-11-08 14:02:11,327 - INFO - Label mapping for Efficiency_Status: {'Low': 0, 'Medium': 1, 'High': 2}
2025-11-08 14:02:11,365 - INFO - Label mapping for Operation_Mode: {'Idle': 0, 'Active': 1, 'Maintenance': 2}
2025-11-08 14:02:11,402 - INFO - Basic data preprocessing completed.
2025-11-08 14:02:11,601 - INFO - Train/test splits and scaler saved successfully.
2025-11-08 14:02:11,605 - INFO - Data processing completed.
```

This confirms that:

* The raw sensor dataset was successfully read and parsed.
* `Timestamp` values were converted to calendar/time components.
* Categorical fields were label-encoded and numerical features scaled.
* Clean, standardised train/test splits and a reusable scaler were saved under `artifacts/processed/`.

## ğŸ§  **Implementation Highlights**

* **Integrated Logging** â€” powered by `src/logger.py`
  Every step is timestamped and recorded for full experiment traceability.

* **Unified Exception Handling** â€” via `src/custom_exception.py`
  Any failure during ingestion, transformation, or scaling raises contextual errors for rapid debugging.

* **Scalable, Modular Design**
  The `DataProcessing` class is reusable and importable â€” easily extended for pipeline orchestration, model training, and deployment workflows.

## ğŸ§© **Integration Guidelines**

| File                      | Purpose                                                                |
| ------------------------- | ---------------------------------------------------------------------- |
| `src/data_processing.py`  | Executes the sensor data preprocessing workflow end-to-end.            |
| `src/custom_exception.py` | Provides structured, contextual exception handling across all modules. |
| `src/logger.py`           | Delivers consistent, timestamped logs for pipeline reproducibility.    |

âœ… **In summary:**
This branch upgrades the repository from a static scaffold into a **fully functional preprocessing stage** of the **MLOps Machine Maintenance** pipeline â€” producing clean, standardised artefacts, traceable logs, and reproducible results that set the foundation for **predictive maintenance modelling, training, and deployment** in later stages.
